{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests as q\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "API_KEY = \"D6427ILDPCC3RZCA\"\n",
    "\n",
    "def _api_call(symbol, candle=\"5min\", output_size=\"compact\"):\n",
    "    url = \"https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol={stock}&interval={skip}&apikey={key}&outputsize={output_size}\".format(\n",
    "        stock=symbol, skip=candle, key=API_KEY, output_size=output_size\n",
    "    )\n",
    "    print(\"API::%s\" % url)\n",
    "    s = q.get(url)\n",
    "    return s.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API::https://www.alphavantage.co/query?function=TIME_SERIES_INTRADAY&symbol=AMZN&interval=5min&apikey=D6427ILDPCC3RZCA&outputsize=full\n",
      "{'1. Information': 'Intraday (5min) prices and volumes', '2. Symbol': 'AMZN', '3. Last Refreshed': '2018-02-05 15:10:00', '4. Interval': '5min', '5. Output Size': 'Full size', '6. Time Zone': 'US/Eastern'}\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "d = _api_call('AMZN', candle=\"5min\", output_size=\"full\")\n",
    "print(d['Meta Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "780"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get time series\n",
    "ts = d['Time Series (5min)']\n",
    "len(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'close': 1391.56,\n",
       " 'high': 1408.82,\n",
       " 'low': 1388.0,\n",
       " 'open': 1404.9601,\n",
       " 'volume': 145334.0}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prep_data(ts):\n",
    "    ts = list(ts.items())\n",
    "    data = []\n",
    "    for _, v in ts:\n",
    "        v = v.items()\n",
    "        data.append({key[3:]: float(val) for key, val in v})\n",
    "    return data\n",
    "            \n",
    "data = prep_data(ts)\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1383.0, 1393.0, 1398.0, 1408.0, 1413.0, 1423.0, 1428.0)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_pivots(p):\n",
    "    pp = (p['high'] + p['low'] + p['close']) / 3\n",
    "    r1 = pp * 2 - p['low']\n",
    "    s1 = pp * 2 - p['high']\n",
    "    r2 = pp + (p['high'] - p['low'])\n",
    "    s2 = pp - (p['high'] - p['low'])\n",
    "    r3 = p['high'] + 2 * (pp - p['low'])\n",
    "    s3 = p['low'] - 2 * (p['high'] - pp)\n",
    "    return s3, s2, s1, pp, r1, r2, r3\n",
    "\n",
    "calc_pivots(data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "779"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = list(range(1, len(data)))\n",
    "pivots = []\n",
    "Y = []\n",
    "for ix in range(1, len(data)):\n",
    "    pivots.append(calc_pivots(data[ix-1]))\n",
    "    Y.append(data[ix]['close'])\n",
    "s3, s2, s1, pp, r1, r2, r3 = list(np.array(pivots).T)\n",
    "len(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYEAAAD8CAYAAACRkhiPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwBhjMtj/AfAQG0B2Yvh0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcb4836d6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X, s1, 'r', X, r1, 'b', X, Y, 'g')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwFarf83/x7sAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcaf11bd4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3IIgCC2GCLcgCEKL8f8BOw+v6/WZ3woAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fcaf115bba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "err = [min(abs(y - r1[i]), abs(y - s1[i])) for i, y in enumerate(Y)]\n",
    "\n",
    "plt.plot(X, err, 'r')\n",
    "plt.show()\n",
    "\n",
    "low_err = [e if e < 10 else 0 for e in err]\n",
    "plt.plot(X, low_err, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LETS MAKE A FUCKING LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, Activation, Dense, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# our input will be the pivot points from the previous candle (7), and formatted to include a timestep\n",
    "model.add(LSTM(1, input_shape=(1, 7), return_sequences=False))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "# model.add(LSTM(64))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('tanh'))\n",
    "model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1471.66666667 1473.72933333 1476.20666667 1487.10333333 1502.20666667\n",
      " 1513.10333333 1528.20666667]\n",
      "(779, 7)\n",
      "(779, 1, 7)\n",
      "[[0.23831253 0.30103826 0.32798918 0.37580332 0.37807407 0.41986928\n",
      "  0.42628207]]\n"
     ]
    }
   ],
   "source": [
    "X = np.asarray(pivots)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(X)\n",
    "print(scaler.data_max_)\n",
    "X_train = scaler.transform(X)\n",
    "print(X_train.shape)\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "print(X_train.shape)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1403.0\n",
      "(779,)\n",
      "(779,)\n",
      "(779,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(779, 2)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = np.asarray(Y)\n",
    "print(Y[0])\n",
    "print(Y.shape)\n",
    "Y_r = Y - r1\n",
    "Y_s = Y - s1\n",
    "print(Y_r.shape)\n",
    "print(Y_s.shape)\n",
    "Y_train = np.asarray((Y_r, Y_s)).T\n",
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 701 samples, validate on 78 samples\n",
      "Epoch 1/1000\n",
      "701/701 [==============================] - 0s 197us/step - loss: 27.0955 - val_loss: 3.0871\n",
      "Epoch 2/1000\n",
      "701/701 [==============================] - 0s 166us/step - loss: 27.0769 - val_loss: 3.0868\n",
      "Epoch 3/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0742 - val_loss: 3.0866\n",
      "Epoch 4/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.1007 - val_loss: 3.0862\n",
      "Epoch 5/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0818 - val_loss: 3.0859\n",
      "Epoch 6/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0809 - val_loss: 3.0857\n",
      "Epoch 7/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0859 - val_loss: 3.0853\n",
      "Epoch 8/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0780 - val_loss: 3.0850\n",
      "Epoch 9/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0890 - val_loss: 3.0847\n",
      "Epoch 10/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0825 - val_loss: 3.0845\n",
      "Epoch 11/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0675 - val_loss: 3.0842\n",
      "Epoch 12/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0858 - val_loss: 3.0840\n",
      "Epoch 13/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0774 - val_loss: 3.0837\n",
      "Epoch 14/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0882 - val_loss: 3.0834\n",
      "Epoch 15/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0829 - val_loss: 3.0832\n",
      "Epoch 16/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0672 - val_loss: 3.0829\n",
      "Epoch 17/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0839 - val_loss: 3.0827\n",
      "Epoch 18/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0761 - val_loss: 3.0824\n",
      "Epoch 19/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0674 - val_loss: 3.0822\n",
      "Epoch 20/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0776 - val_loss: 3.0819\n",
      "Epoch 21/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0669 - val_loss: 3.0817\n",
      "Epoch 22/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0796 - val_loss: 3.0814\n",
      "Epoch 23/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0651 - val_loss: 3.0812\n",
      "Epoch 24/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0727 - val_loss: 3.0810\n",
      "Epoch 25/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0693 - val_loss: 3.0808\n",
      "Epoch 26/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0727 - val_loss: 3.0806\n",
      "Epoch 27/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0657 - val_loss: 3.0803\n",
      "Epoch 28/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0658 - val_loss: 3.0801\n",
      "Epoch 29/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0662 - val_loss: 3.0799\n",
      "Epoch 30/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0772 - val_loss: 3.0797\n",
      "Epoch 31/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0681 - val_loss: 3.0795\n",
      "Epoch 32/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0658 - val_loss: 3.0794\n",
      "Epoch 33/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0689 - val_loss: 3.0792\n",
      "Epoch 34/1000\n",
      "701/701 [==============================] - 0s 156us/step - loss: 27.0630 - val_loss: 3.0790\n",
      "Epoch 35/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0514 - val_loss: 3.0788\n",
      "Epoch 36/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0758 - val_loss: 3.0786\n",
      "Epoch 37/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0577 - val_loss: 3.0785\n",
      "Epoch 38/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0527 - val_loss: 3.0783\n",
      "Epoch 39/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0654 - val_loss: 3.0781\n",
      "Epoch 40/1000\n",
      "701/701 [==============================] - 0s 159us/step - loss: 27.0638 - val_loss: 3.0780\n",
      "Epoch 41/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0641 - val_loss: 3.0778\n",
      "Epoch 42/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0674 - val_loss: 3.0776\n",
      "Epoch 43/1000\n",
      "701/701 [==============================] - 0s 162us/step - loss: 27.0615 - val_loss: 3.0775\n",
      "Epoch 44/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0504 - val_loss: 3.0773\n",
      "Epoch 45/1000\n",
      "701/701 [==============================] - 0s 158us/step - loss: 27.0473 - val_loss: 3.0772\n",
      "Epoch 46/1000\n",
      "701/701 [==============================] - 0s 177us/step - loss: 27.0588 - val_loss: 3.0771\n",
      "Epoch 47/1000\n",
      "701/701 [==============================] - 0s 162us/step - loss: 27.0592 - val_loss: 3.0769\n",
      "Epoch 48/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0591 - val_loss: 3.0768\n",
      "Epoch 49/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0544 - val_loss: 3.0766\n",
      "Epoch 50/1000\n",
      "701/701 [==============================] - 0s 163us/step - loss: 27.0478 - val_loss: 3.0765\n",
      "Epoch 51/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0548 - val_loss: 3.0764\n",
      "Epoch 52/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0486 - val_loss: 3.0763\n",
      "Epoch 53/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0451 - val_loss: 3.0762\n",
      "Epoch 54/1000\n",
      "701/701 [==============================] - 0s 161us/step - loss: 27.0545 - val_loss: 3.0760\n",
      "Epoch 55/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0508 - val_loss: 3.0759\n",
      "Epoch 56/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0470 - val_loss: 3.0758\n",
      "Epoch 57/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0507 - val_loss: 3.0757\n",
      "Epoch 58/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0434 - val_loss: 3.0756\n",
      "Epoch 59/1000\n",
      "701/701 [==============================] - 0s 156us/step - loss: 27.0502 - val_loss: 3.0755\n",
      "Epoch 60/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0455 - val_loss: 3.0754\n",
      "Epoch 61/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0457 - val_loss: 3.0753\n",
      "Epoch 62/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0478 - val_loss: 3.0752\n",
      "Epoch 63/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0372 - val_loss: 3.0751\n",
      "Epoch 64/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0450 - val_loss: 3.0750\n",
      "Epoch 65/1000\n",
      "701/701 [==============================] - 0s 160us/step - loss: 27.0435 - val_loss: 3.0749\n",
      "Epoch 66/1000\n",
      "701/701 [==============================] - 0s 164us/step - loss: 27.0388 - val_loss: 3.0748\n",
      "Epoch 67/1000\n",
      "701/701 [==============================] - 0s 163us/step - loss: 27.0391 - val_loss: 3.0748\n",
      "Epoch 68/1000\n",
      "701/701 [==============================] - 0s 168us/step - loss: 27.0448 - val_loss: 3.0747\n",
      "Epoch 69/1000\n",
      "701/701 [==============================] - 0s 164us/step - loss: 27.0434 - val_loss: 3.0746\n",
      "Epoch 70/1000\n",
      "701/701 [==============================] - 0s 162us/step - loss: 27.0411 - val_loss: 3.0745\n",
      "Epoch 71/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0413 - val_loss: 3.0744\n",
      "Epoch 72/1000\n",
      "701/701 [==============================] - 0s 167us/step - loss: 27.0384 - val_loss: 3.0743\n",
      "Epoch 73/1000\n",
      "701/701 [==============================] - 0s 160us/step - loss: 27.0438 - val_loss: 3.0743\n",
      "Epoch 74/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0390 - val_loss: 3.0742\n",
      "Epoch 75/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0428 - val_loss: 3.0741\n",
      "Epoch 76/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0387 - val_loss: 3.0740\n",
      "Epoch 77/1000\n",
      "701/701 [==============================] - 0s 164us/step - loss: 27.0390 - val_loss: 3.0740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0408 - val_loss: 3.0739\n",
      "Epoch 79/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0395 - val_loss: 3.0738\n",
      "Epoch 80/1000\n",
      "701/701 [==============================] - 0s 142us/step - loss: 27.0342 - val_loss: 3.0738\n",
      "Epoch 81/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0337 - val_loss: 3.0737\n",
      "Epoch 82/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0363 - val_loss: 3.0736\n",
      "Epoch 83/1000\n",
      "701/701 [==============================] - 0s 171us/step - loss: 27.0387 - val_loss: 3.0736\n",
      "Epoch 84/1000\n",
      "701/701 [==============================] - 0s 174us/step - loss: 27.0354 - val_loss: 3.0735\n",
      "Epoch 85/1000\n",
      "701/701 [==============================] - 0s 174us/step - loss: 27.0306 - val_loss: 3.0735\n",
      "Epoch 86/1000\n",
      "701/701 [==============================] - 0s 180us/step - loss: 27.0322 - val_loss: 3.0734\n",
      "Epoch 87/1000\n",
      "701/701 [==============================] - 0s 174us/step - loss: 27.0337 - val_loss: 3.0733\n",
      "Epoch 88/1000\n",
      "701/701 [==============================] - 0s 172us/step - loss: 27.0330 - val_loss: 3.0733\n",
      "Epoch 89/1000\n",
      "701/701 [==============================] - 0s 172us/step - loss: 27.0350 - val_loss: 3.0732\n",
      "Epoch 90/1000\n",
      "701/701 [==============================] - 0s 170us/step - loss: 27.0313 - val_loss: 3.0732\n",
      "Epoch 91/1000\n",
      "701/701 [==============================] - 0s 177us/step - loss: 27.0275 - val_loss: 3.0732\n",
      "Epoch 92/1000\n",
      "701/701 [==============================] - 0s 175us/step - loss: 27.0307 - val_loss: 3.0731\n",
      "Epoch 93/1000\n",
      "701/701 [==============================] - 0s 173us/step - loss: 27.0266 - val_loss: 3.0731\n",
      "Epoch 94/1000\n",
      "701/701 [==============================] - 0s 183us/step - loss: 27.0337 - val_loss: 3.0730\n",
      "Epoch 95/1000\n",
      "701/701 [==============================] - 0s 175us/step - loss: 27.0300 - val_loss: 3.0730\n",
      "Epoch 96/1000\n",
      "701/701 [==============================] - 0s 175us/step - loss: 27.0336 - val_loss: 3.0729\n",
      "Epoch 97/1000\n",
      "701/701 [==============================] - 0s 177us/step - loss: 27.0343 - val_loss: 3.0729\n",
      "Epoch 98/1000\n",
      "701/701 [==============================] - 0s 175us/step - loss: 27.0295 - val_loss: 3.0728\n",
      "Epoch 99/1000\n",
      "701/701 [==============================] - 0s 175us/step - loss: 27.0293 - val_loss: 3.0728\n",
      "Epoch 100/1000\n",
      "701/701 [==============================] - 0s 175us/step - loss: 27.0274 - val_loss: 3.0728\n",
      "Epoch 101/1000\n",
      "701/701 [==============================] - 0s 174us/step - loss: 27.0304 - val_loss: 3.0727\n",
      "Epoch 102/1000\n",
      "701/701 [==============================] - 0s 177us/step - loss: 27.0308 - val_loss: 3.0727\n",
      "Epoch 103/1000\n",
      "701/701 [==============================] - 0s 169us/step - loss: 27.0288 - val_loss: 3.0726\n",
      "Epoch 104/1000\n",
      "701/701 [==============================] - 0s 174us/step - loss: 27.0276 - val_loss: 3.0726\n",
      "Epoch 105/1000\n",
      "701/701 [==============================] - 0s 175us/step - loss: 27.0297 - val_loss: 3.0726\n",
      "Epoch 106/1000\n",
      "701/701 [==============================] - 0s 179us/step - loss: 27.0300 - val_loss: 3.0725\n",
      "Epoch 107/1000\n",
      "701/701 [==============================] - 0s 177us/step - loss: 27.0281 - val_loss: 3.0725\n",
      "Epoch 108/1000\n",
      "701/701 [==============================] - 0s 182us/step - loss: 27.0282 - val_loss: 3.0725\n",
      "Epoch 109/1000\n",
      "701/701 [==============================] - 0s 174us/step - loss: 27.0255 - val_loss: 3.0724\n",
      "Epoch 110/1000\n",
      "701/701 [==============================] - 0s 183us/step - loss: 27.0252 - val_loss: 3.0724\n",
      "Epoch 111/1000\n",
      "701/701 [==============================] - 0s 180us/step - loss: 27.0312 - val_loss: 3.0724\n",
      "Epoch 112/1000\n",
      "701/701 [==============================] - 0s 176us/step - loss: 27.0262 - val_loss: 3.0723\n",
      "Epoch 113/1000\n",
      "701/701 [==============================] - 0s 188us/step - loss: 27.0268 - val_loss: 3.0723\n",
      "Epoch 114/1000\n",
      "701/701 [==============================] - 0s 177us/step - loss: 27.0310 - val_loss: 3.0723\n",
      "Epoch 115/1000\n",
      "701/701 [==============================] - 0s 178us/step - loss: 27.0262 - val_loss: 3.0722\n",
      "Epoch 116/1000\n",
      "701/701 [==============================] - 0s 168us/step - loss: 27.0243 - val_loss: 3.0722\n",
      "Epoch 117/1000\n",
      "701/701 [==============================] - 0s 164us/step - loss: 27.0254 - val_loss: 3.0722\n",
      "Epoch 118/1000\n",
      "701/701 [==============================] - 0s 164us/step - loss: 27.0300 - val_loss: 3.0721\n",
      "Epoch 119/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0229 - val_loss: 3.0721\n",
      "Epoch 120/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0210 - val_loss: 3.0721\n",
      "Epoch 121/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0267 - val_loss: 3.0721\n",
      "Epoch 122/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0247 - val_loss: 3.0721\n",
      "Epoch 123/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0240 - val_loss: 3.0720\n",
      "Epoch 124/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0262 - val_loss: 3.0720\n",
      "Epoch 125/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0247 - val_loss: 3.0720\n",
      "Epoch 126/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0233 - val_loss: 3.0720\n",
      "Epoch 127/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0264 - val_loss: 3.0719\n",
      "Epoch 128/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0241 - val_loss: 3.0719\n",
      "Epoch 129/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0212 - val_loss: 3.0719\n",
      "Epoch 130/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0225 - val_loss: 3.0719\n",
      "Epoch 131/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0222 - val_loss: 3.0719\n",
      "Epoch 132/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0240 - val_loss: 3.0718\n",
      "Epoch 133/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0230 - val_loss: 3.0718\n",
      "Epoch 134/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0243 - val_loss: 3.0718\n",
      "Epoch 135/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0232 - val_loss: 3.0718\n",
      "Epoch 136/1000\n",
      "701/701 [==============================] - 0s 165us/step - loss: 27.0209 - val_loss: 3.0718\n",
      "Epoch 137/1000\n",
      "701/701 [==============================] - 0s 156us/step - loss: 27.0227 - val_loss: 3.0717\n",
      "Epoch 138/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0218 - val_loss: 3.0717\n",
      "Epoch 139/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0228 - val_loss: 3.0717\n",
      "Epoch 140/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0200 - val_loss: 3.0717\n",
      "Epoch 141/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0210 - val_loss: 3.0717\n",
      "Epoch 142/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0202 - val_loss: 3.0717\n",
      "Epoch 143/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0207 - val_loss: 3.0717\n",
      "Epoch 144/1000\n",
      "701/701 [==============================] - 0s 158us/step - loss: 27.0210 - val_loss: 3.0716\n",
      "Epoch 145/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0213 - val_loss: 3.0716\n",
      "Epoch 146/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0196 - val_loss: 3.0716\n",
      "Epoch 147/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0191 - val_loss: 3.0716\n",
      "Epoch 148/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0200 - val_loss: 3.0716\n",
      "Epoch 149/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0193 - val_loss: 3.0716\n",
      "Epoch 150/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0194 - val_loss: 3.0716\n",
      "Epoch 151/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0201 - val_loss: 3.0715\n",
      "Epoch 152/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0185 - val_loss: 3.0715\n",
      "Epoch 153/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0203 - val_loss: 3.0715\n",
      "Epoch 154/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0195 - val_loss: 3.0715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/1000\n",
      "701/701 [==============================] - 0s 160us/step - loss: 27.0180 - val_loss: 3.0715\n",
      "Epoch 156/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0188 - val_loss: 3.0715\n",
      "Epoch 157/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0198 - val_loss: 3.0715\n",
      "Epoch 158/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0182 - val_loss: 3.0715\n",
      "Epoch 159/1000\n",
      "701/701 [==============================] - 0s 156us/step - loss: 27.0181 - val_loss: 3.0715\n",
      "Epoch 160/1000\n",
      "701/701 [==============================] - 0s 142us/step - loss: 27.0165 - val_loss: 3.0715\n",
      "Epoch 161/1000\n",
      "701/701 [==============================] - 0s 143us/step - loss: 27.0191 - val_loss: 3.0714\n",
      "Epoch 162/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0196 - val_loss: 3.0714\n",
      "Epoch 163/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0179 - val_loss: 3.0714\n",
      "Epoch 164/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0166 - val_loss: 3.0714\n",
      "Epoch 165/1000\n",
      "701/701 [==============================] - 0s 142us/step - loss: 27.0165 - val_loss: 3.0714\n",
      "Epoch 166/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0192 - val_loss: 3.0714\n",
      "Epoch 167/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0177 - val_loss: 3.0714\n",
      "Epoch 168/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0171 - val_loss: 3.0714\n",
      "Epoch 169/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0185 - val_loss: 3.0714\n",
      "Epoch 170/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0177 - val_loss: 3.0714\n",
      "Epoch 171/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0176 - val_loss: 3.0714\n",
      "Epoch 172/1000\n",
      "701/701 [==============================] - 0s 158us/step - loss: 27.0172 - val_loss: 3.0714\n",
      "Epoch 173/1000\n",
      "701/701 [==============================] - 0s 157us/step - loss: 27.0174 - val_loss: 3.0713\n",
      "Epoch 174/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0166 - val_loss: 3.0713\n",
      "Epoch 175/1000\n",
      "701/701 [==============================] - 0s 156us/step - loss: 27.0181 - val_loss: 3.0713\n",
      "Epoch 176/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0158 - val_loss: 3.0713\n",
      "Epoch 177/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0168 - val_loss: 3.0713\n",
      "Epoch 178/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0162 - val_loss: 3.0713\n",
      "Epoch 179/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0167 - val_loss: 3.0713\n",
      "Epoch 180/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0165 - val_loss: 3.0713\n",
      "Epoch 181/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0155 - val_loss: 3.0713\n",
      "Epoch 182/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0158 - val_loss: 3.0713\n",
      "Epoch 183/1000\n",
      "701/701 [==============================] - 0s 165us/step - loss: 27.0168 - val_loss: 3.0713\n",
      "Epoch 184/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0164 - val_loss: 3.0713\n",
      "Epoch 185/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0171 - val_loss: 3.0713\n",
      "Epoch 186/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0153 - val_loss: 3.0713\n",
      "Epoch 187/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0162 - val_loss: 3.0713\n",
      "Epoch 188/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0164 - val_loss: 3.0713\n",
      "Epoch 189/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0170 - val_loss: 3.0713\n",
      "Epoch 190/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0167 - val_loss: 3.0712\n",
      "Epoch 191/1000\n",
      "701/701 [==============================] - 0s 161us/step - loss: 27.0153 - val_loss: 3.0712\n",
      "Epoch 192/1000\n",
      "701/701 [==============================] - 0s 163us/step - loss: 27.0155 - val_loss: 3.0712\n",
      "Epoch 193/1000\n",
      "701/701 [==============================] - 0s 159us/step - loss: 27.0154 - val_loss: 3.0712\n",
      "Epoch 194/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0157 - val_loss: 3.0712\n",
      "Epoch 195/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0155 - val_loss: 3.0712\n",
      "Epoch 196/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0158 - val_loss: 3.0712\n",
      "Epoch 197/1000\n",
      "701/701 [==============================] - 0s 163us/step - loss: 27.0153 - val_loss: 3.0712\n",
      "Epoch 198/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0162 - val_loss: 3.0712\n",
      "Epoch 199/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0159 - val_loss: 3.0712\n",
      "Epoch 200/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0149 - val_loss: 3.0712\n",
      "Epoch 201/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0154 - val_loss: 3.0712\n",
      "Epoch 202/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0151 - val_loss: 3.0712\n",
      "Epoch 203/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0153 - val_loss: 3.0712\n",
      "Epoch 204/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0151 - val_loss: 3.0712\n",
      "Epoch 205/1000\n",
      "701/701 [==============================] - 0s 164us/step - loss: 27.0148 - val_loss: 3.0712\n",
      "Epoch 206/1000\n",
      "701/701 [==============================] - 0s 160us/step - loss: 27.0154 - val_loss: 3.0712\n",
      "Epoch 207/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0151 - val_loss: 3.0712\n",
      "Epoch 208/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0149 - val_loss: 3.0712\n",
      "Epoch 209/1000\n",
      "701/701 [==============================] - 0s 158us/step - loss: 27.0152 - val_loss: 3.0712\n",
      "Epoch 210/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0143 - val_loss: 3.0712\n",
      "Epoch 211/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0149 - val_loss: 3.0712\n",
      "Epoch 212/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0144 - val_loss: 3.0712\n",
      "Epoch 213/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0156 - val_loss: 3.0712\n",
      "Epoch 214/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0150 - val_loss: 3.0712\n",
      "Epoch 215/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0146 - val_loss: 3.0712\n",
      "Epoch 216/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0146 - val_loss: 3.0712\n",
      "Epoch 217/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0150 - val_loss: 3.0711\n",
      "Epoch 218/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0145 - val_loss: 3.0711\n",
      "Epoch 219/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0146 - val_loss: 3.0711\n",
      "Epoch 220/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0141 - val_loss: 3.0711\n",
      "Epoch 221/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0146 - val_loss: 3.0711\n",
      "Epoch 222/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0140 - val_loss: 3.0711\n",
      "Epoch 223/1000\n",
      "701/701 [==============================] - 0s 157us/step - loss: 27.0144 - val_loss: 3.0711\n",
      "Epoch 224/1000\n",
      "701/701 [==============================] - 0s 162us/step - loss: 27.0142 - val_loss: 3.0711\n",
      "Epoch 225/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0143 - val_loss: 3.0711\n",
      "Epoch 226/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0140 - val_loss: 3.0711\n",
      "Epoch 227/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0149 - val_loss: 3.0711\n",
      "Epoch 228/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0145 - val_loss: 3.0711\n",
      "Epoch 229/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0144 - val_loss: 3.0711\n",
      "Epoch 230/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0138 - val_loss: 3.0711\n",
      "Epoch 231/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701/701 [==============================] - 0s 149us/step - loss: 27.0141 - val_loss: 3.0711\n",
      "Epoch 232/1000\n",
      "701/701 [==============================] - 0s 142us/step - loss: 27.0142 - val_loss: 3.0711\n",
      "Epoch 233/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0141 - val_loss: 3.0711\n",
      "Epoch 234/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0138 - val_loss: 3.0711\n",
      "Epoch 235/1000\n",
      "701/701 [==============================] - 0s 158us/step - loss: 27.0140 - val_loss: 3.0711\n",
      "Epoch 236/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0139 - val_loss: 3.0711\n",
      "Epoch 237/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0143 - val_loss: 3.0711\n",
      "Epoch 238/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0138 - val_loss: 3.0711\n",
      "Epoch 239/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0141 - val_loss: 3.0711\n",
      "Epoch 240/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0137 - val_loss: 3.0711\n",
      "Epoch 241/1000\n",
      "701/701 [==============================] - 0s 167us/step - loss: 27.0140 - val_loss: 3.0711\n",
      "Epoch 242/1000\n",
      "701/701 [==============================] - 0s 172us/step - loss: 27.0137 - val_loss: 3.0711\n",
      "Epoch 243/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0136 - val_loss: 3.0711\n",
      "Epoch 244/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0134 - val_loss: 3.0711\n",
      "Epoch 245/1000\n",
      "701/701 [==============================] - 0s 142us/step - loss: 27.0134 - val_loss: 3.0711\n",
      "Epoch 246/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0138 - val_loss: 3.0711\n",
      "Epoch 247/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0134 - val_loss: 3.0711\n",
      "Epoch 248/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0136 - val_loss: 3.0711\n",
      "Epoch 249/1000\n",
      "701/701 [==============================] - 0s 143us/step - loss: 27.0135 - val_loss: 3.0711\n",
      "Epoch 250/1000\n",
      "701/701 [==============================] - 0s 161us/step - loss: 27.0139 - val_loss: 3.0711\n",
      "Epoch 251/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0132 - val_loss: 3.0711\n",
      "Epoch 252/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0134 - val_loss: 3.0711\n",
      "Epoch 253/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0137 - val_loss: 3.0711\n",
      "Epoch 254/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0137 - val_loss: 3.0711\n",
      "Epoch 255/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0135 - val_loss: 3.0711\n",
      "Epoch 256/1000\n",
      "701/701 [==============================] - 0s 161us/step - loss: 27.0135 - val_loss: 3.0711\n",
      "Epoch 257/1000\n",
      "701/701 [==============================] - 0s 165us/step - loss: 27.0131 - val_loss: 3.0711\n",
      "Epoch 258/1000\n",
      "701/701 [==============================] - 0s 168us/step - loss: 27.0134 - val_loss: 3.0711\n",
      "Epoch 259/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0131 - val_loss: 3.0711\n",
      "Epoch 260/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0135 - val_loss: 3.0711\n",
      "Epoch 261/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0130 - val_loss: 3.0711\n",
      "Epoch 262/1000\n",
      "701/701 [==============================] - 0s 160us/step - loss: 27.0131 - val_loss: 3.0711\n",
      "Epoch 263/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0133 - val_loss: 3.0711\n",
      "Epoch 264/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0134 - val_loss: 3.0711\n",
      "Epoch 265/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0133 - val_loss: 3.0711\n",
      "Epoch 266/1000\n",
      "701/701 [==============================] - 0s 156us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 267/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0130 - val_loss: 3.0711\n",
      "Epoch 268/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0132 - val_loss: 3.0711\n",
      "Epoch 269/1000\n",
      "701/701 [==============================] - 0s 156us/step - loss: 27.0130 - val_loss: 3.0711\n",
      "Epoch 270/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0130 - val_loss: 3.0711\n",
      "Epoch 271/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 272/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0129 - val_loss: 3.0711\n",
      "Epoch 273/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0133 - val_loss: 3.0711\n",
      "Epoch 274/1000\n",
      "701/701 [==============================] - 0s 160us/step - loss: 27.0130 - val_loss: 3.0711\n",
      "Epoch 275/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0130 - val_loss: 3.0711\n",
      "Epoch 276/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0130 - val_loss: 3.0711\n",
      "Epoch 277/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0130 - val_loss: 3.0711\n",
      "Epoch 278/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0129 - val_loss: 3.0711\n",
      "Epoch 279/1000\n",
      "701/701 [==============================] - 0s 159us/step - loss: 27.0130 - val_loss: 3.0711\n",
      "Epoch 280/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0129 - val_loss: 3.0711\n",
      "Epoch 281/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 282/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 283/1000\n",
      "701/701 [==============================] - 0s 157us/step - loss: 27.0131 - val_loss: 3.0711\n",
      "Epoch 284/1000\n",
      "701/701 [==============================] - 0s 159us/step - loss: 27.0129 - val_loss: 3.0711\n",
      "Epoch 285/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0129 - val_loss: 3.0711\n",
      "Epoch 286/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0130 - val_loss: 3.0711\n",
      "Epoch 287/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0129 - val_loss: 3.0711\n",
      "Epoch 288/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0130 - val_loss: 3.0711\n",
      "Epoch 289/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 290/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0129 - val_loss: 3.0711\n",
      "Epoch 291/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 292/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0129 - val_loss: 3.0711\n",
      "Epoch 293/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0129 - val_loss: 3.0711\n",
      "Epoch 294/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0127 - val_loss: 3.0711\n",
      "Epoch 295/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 296/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 297/1000\n",
      "701/701 [==============================] - 0s 159us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 298/1000\n",
      "701/701 [==============================] - 0s 166us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 299/1000\n",
      "701/701 [==============================] - 0s 157us/step - loss: 27.0126 - val_loss: 3.0711\n",
      "Epoch 300/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 301/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 302/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 303/1000\n",
      "701/701 [==============================] - 0s 164us/step - loss: 27.0126 - val_loss: 3.0711\n",
      "Epoch 304/1000\n",
      "701/701 [==============================] - 0s 161us/step - loss: 27.0126 - val_loss: 3.0711\n",
      "Epoch 305/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0127 - val_loss: 3.0711\n",
      "Epoch 306/1000\n",
      "701/701 [==============================] - 0s 156us/step - loss: 27.0126 - val_loss: 3.0711\n",
      "Epoch 307/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701/701 [==============================] - 0s 150us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 308/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0127 - val_loss: 3.0711\n",
      "Epoch 309/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0128 - val_loss: 3.0711\n",
      "Epoch 310/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0126 - val_loss: 3.0710\n",
      "Epoch 311/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 312/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0127 - val_loss: 3.0710\n",
      "Epoch 313/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0127 - val_loss: 3.0710\n",
      "Epoch 314/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0126 - val_loss: 3.0710\n",
      "Epoch 315/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0126 - val_loss: 3.0710\n",
      "Epoch 316/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0127 - val_loss: 3.0710\n",
      "Epoch 317/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 318/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0126 - val_loss: 3.0710\n",
      "Epoch 319/1000\n",
      "701/701 [==============================] - 0s 156us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 320/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0126 - val_loss: 3.0710\n",
      "Epoch 321/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0126 - val_loss: 3.0710\n",
      "Epoch 322/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0126 - val_loss: 3.0710\n",
      "Epoch 323/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 324/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 325/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 326/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 327/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 328/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 329/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0126 - val_loss: 3.0710\n",
      "Epoch 330/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 331/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 332/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 333/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 334/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 335/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 336/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 337/1000\n",
      "701/701 [==============================] - 0s 142us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 338/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 339/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 340/1000\n",
      "701/701 [==============================] - 0s 157us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 341/1000\n",
      "701/701 [==============================] - 0s 161us/step - loss: 27.0125 - val_loss: 3.0710\n",
      "Epoch 342/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 343/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 344/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 345/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 346/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 347/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 348/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 349/1000\n",
      "701/701 [==============================] - 0s 158us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 350/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 351/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 352/1000\n",
      "701/701 [==============================] - 0s 160us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 353/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 354/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 355/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 356/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 357/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 358/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 359/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 360/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 361/1000\n",
      "701/701 [==============================] - 0s 156us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 362/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 363/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 364/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 365/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 366/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 367/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 368/1000\n",
      "701/701 [==============================] - 0s 160us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 369/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 370/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0124 - val_loss: 3.0710\n",
      "Epoch 371/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 372/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 373/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 374/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 375/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 376/1000\n",
      "701/701 [==============================] - 0s 143us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 377/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 378/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 379/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 380/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 381/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 382/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 383/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701/701 [==============================] - 0s 152us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 384/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 385/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 386/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 387/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 388/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 389/1000\n",
      "701/701 [==============================] - 0s 140us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 390/1000\n",
      "701/701 [==============================] - 0s 139us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 391/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 392/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 393/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 394/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 395/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 396/1000\n",
      "701/701 [==============================] - 0s 160us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 397/1000\n",
      "701/701 [==============================] - 0s 177us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 398/1000\n",
      "701/701 [==============================] - 0s 171us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 399/1000\n",
      "701/701 [==============================] - 0s 175us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 400/1000\n",
      "701/701 [==============================] - 0s 176us/step - loss: 27.0123 - val_loss: 3.0710\n",
      "Epoch 401/1000\n",
      "701/701 [==============================] - 0s 179us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 402/1000\n",
      "701/701 [==============================] - 0s 174us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 403/1000\n",
      "701/701 [==============================] - 0s 178us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 404/1000\n",
      "701/701 [==============================] - 0s 184us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 405/1000\n",
      "701/701 [==============================] - 0s 175us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 406/1000\n",
      "701/701 [==============================] - 0s 175us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 407/1000\n",
      "701/701 [==============================] - 0s 179us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 408/1000\n",
      "701/701 [==============================] - 0s 182us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 409/1000\n",
      "701/701 [==============================] - ETA: 0s - loss: 17.04 - 0s 177us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 410/1000\n",
      "701/701 [==============================] - 0s 176us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 411/1000\n",
      "701/701 [==============================] - 0s 178us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 412/1000\n",
      "701/701 [==============================] - 0s 177us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 413/1000\n",
      "701/701 [==============================] - 0s 171us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 414/1000\n",
      "701/701 [==============================] - 0s 158us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 415/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 416/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 417/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 418/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 419/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 420/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 421/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 422/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 423/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 424/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 425/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 426/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 427/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 428/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 429/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 430/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 431/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 432/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 433/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 434/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 435/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 436/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 437/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 438/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 439/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 440/1000\n",
      "701/701 [==============================] - 0s 157us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 441/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 442/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 443/1000\n",
      "701/701 [==============================] - 0s 143us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 444/1000\n",
      "701/701 [==============================] - 0s 163us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 445/1000\n",
      "701/701 [==============================] - 0s 166us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 446/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 447/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 448/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 449/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 450/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 451/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 452/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 453/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 454/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 455/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 456/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 457/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 458/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 459/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701/701 [==============================] - 0s 162us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 460/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 461/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 462/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 463/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 464/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 465/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 466/1000\n",
      "701/701 [==============================] - 0s 142us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 467/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 468/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 469/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 470/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 471/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 472/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 473/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 474/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 475/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 476/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 477/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 478/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 479/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 480/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0122 - val_loss: 3.0710\n",
      "Epoch 481/1000\n",
      "701/701 [==============================] - 0s 164us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 482/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 483/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 484/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 485/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 486/1000\n",
      "701/701 [==============================] - 0s 155us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 487/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 488/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 489/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 490/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 491/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 492/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 493/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 494/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 495/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 496/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 497/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 498/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 499/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 500/1000\n",
      "701/701 [==============================] - 0s 160us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 501/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 502/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 503/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 504/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 505/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 506/1000\n",
      "701/701 [==============================] - 0s 145us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 507/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 508/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 509/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 510/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 511/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 512/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 513/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 514/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 515/1000\n",
      "701/701 [==============================] - 0s 159us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 516/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 517/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 518/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 519/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 520/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 521/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 522/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 523/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 524/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 525/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 526/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 527/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 528/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 529/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 530/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 531/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 532/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 533/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 534/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 535/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701/701 [==============================] - 0s 156us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 536/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 537/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 538/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 539/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 540/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 541/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 542/1000\n",
      "701/701 [==============================] - 0s 156us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 543/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 544/1000\n",
      "701/701 [==============================] - 0s 159us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 545/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 546/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 547/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 548/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 549/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 550/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 551/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 552/1000\n",
      "701/701 [==============================] - 0s 157us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 553/1000\n",
      "701/701 [==============================] - 0s 161us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 554/1000\n",
      "701/701 [==============================] - 0s 163us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 555/1000\n",
      "701/701 [==============================] - 0s 165us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 556/1000\n",
      "701/701 [==============================] - 0s 164us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 557/1000\n",
      "701/701 [==============================] - 0s 160us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 558/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 559/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 560/1000\n",
      "701/701 [==============================] - 0s 143us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 561/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 562/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 563/1000\n",
      "701/701 [==============================] - 0s 143us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 564/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 565/1000\n",
      "701/701 [==============================] - 0s 140us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 566/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 567/1000\n",
      "701/701 [==============================] - 0s 142us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 568/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 569/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 570/1000\n",
      "701/701 [==============================] - 0s 187us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 571/1000\n",
      "701/701 [==============================] - 0s 173us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 572/1000\n",
      "701/701 [==============================] - 0s 161us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 573/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 574/1000\n",
      "701/701 [==============================] - 0s 156us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 575/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 576/1000\n",
      "701/701 [==============================] - 0s 157us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 577/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 578/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 579/1000\n",
      "701/701 [==============================] - 0s 159us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 580/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 581/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 582/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 583/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 584/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 585/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 586/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 587/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 588/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 589/1000\n",
      "701/701 [==============================] - 0s 163us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 590/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 591/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 592/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 593/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 594/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 595/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 596/1000\n",
      "701/701 [==============================] - 0s 146us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 597/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 598/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 599/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 600/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 601/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 602/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 603/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 604/1000\n",
      "701/701 [==============================] - 0s 169us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 605/1000\n",
      "701/701 [==============================] - 0s 178us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 606/1000\n",
      "701/701 [==============================] - 0s 175us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 607/1000\n",
      "701/701 [==============================] - 0s 190us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 608/1000\n",
      "701/701 [==============================] - 0s 178us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 609/1000\n",
      "701/701 [==============================] - 0s 179us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 610/1000\n",
      "701/701 [==============================] - 0s 182us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 611/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701/701 [==============================] - 0s 176us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 612/1000\n",
      "701/701 [==============================] - 0s 172us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 613/1000\n",
      "701/701 [==============================] - 0s 171us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 614/1000\n",
      "701/701 [==============================] - 0s 148us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 615/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 616/1000\n",
      "701/701 [==============================] - 0s 149us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 617/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 618/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 619/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 620/1000\n",
      "701/701 [==============================] - 0s 147us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 621/1000\n",
      "701/701 [==============================] - 0s 151us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 622/1000\n",
      "701/701 [==============================] - 0s 153us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 623/1000\n",
      "701/701 [==============================] - 0s 152us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 624/1000\n",
      "701/701 [==============================] - 0s 154us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 625/1000\n",
      "701/701 [==============================] - 0s 144us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 626/1000\n",
      "701/701 [==============================] - 0s 150us/step - loss: 27.0121 - val_loss: 3.0710\n",
      "Epoch 627/1000\n",
      " 16/701 [..............................] - ETA: 0s - loss: 19.9795"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-72c354811cb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2353\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2354\u001b[0m         \u001b[0mfetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdates_op\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2355\u001b[0;31m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m   2357\u001b[0m                               **self.session_kwargs)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mcandidate_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36mglobal_variables\u001b[0;34m(scope)\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0mA\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m   \"\"\"\n\u001b[0;32m-> 1247\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_VARIABLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_collection\u001b[0;34m(key, scope)\u001b[0m\n\u001b[1;32m   4850\u001b[0m     \u001b[0mcollected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4851\u001b[0m   \"\"\"\n\u001b[0;32m-> 4852\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_collection\u001b[0;34m(self, name, scope)\u001b[0m\n\u001b[1;32m   3364\u001b[0m     \"\"\"  # pylint: disable=g-doc-exception\n\u001b[1;32m   3365\u001b[0m     \u001b[0m_assert_collection_is_ok\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3366\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3367\u001b[0m       \u001b[0mcollection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3368\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcollection\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=1000, batch_size=16, shuffle=True, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(779, 2)\n"
     ]
    }
   ],
   "source": [
    "Yhat = model.predict(X_train)\n",
    "print(Yhat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profit: 3.991803\n",
      "Profit: 17.063647\n",
      "Profit: 18.167902\n",
      "Profit: 21.603943\n",
      "Profit: 23.192532\n",
      "Profit: 23.693789\n",
      "Profit: 24.118147\n",
      "Profit: 24.520673\n",
      "Profit: 26.968093\n",
      "Profit: 28.275811\n",
      "Profit: 31.590818\n",
      "Profit: 32.767557\n",
      "Profit: 36.728821\n",
      "Profit: 38.633822\n",
      "Profit: 40.830407\n",
      "Profit: 41.935016\n",
      "Profit: 44.979475\n",
      "Profit: 45.036878\n",
      "Profit: 50.653575\n",
      "Profit: 51.581314\n",
      "Profit: 53.672973\n",
      "Profit: 54.371721\n",
      "Profit: 54.651738\n",
      "Profit: 54.677251\n",
      "Profit: 55.385567\n",
      "Profit: 57.407027\n",
      "Profit: 60.450760\n",
      "Profit: 62.895275\n",
      "Profit: 72.360430\n"
     ]
    }
   ],
   "source": [
    "buy = True\n",
    "capital = 1000\n",
    "usd = capital\n",
    "stock = 0\n",
    "profit = []\n",
    "buy_thresh = 5\n",
    "sell_thresh = 1\n",
    "for i, x in enumerate(Y):\n",
    "    buy_dist, sell_dist = Yhat[i]\n",
    "    if abs(buy_dist) < buy_thresh and buy:\n",
    "        stock = usd / x\n",
    "        usd = 0\n",
    "        buy = False\n",
    "    elif abs(sell_dist) < sell_thresh and not buy:\n",
    "        n_usd = stock * x\n",
    "        if len(profit) > 0 and n_usd - capital < profit[-1]:\n",
    "            continue\n",
    "        usd = n_usd\n",
    "        stock = 0\n",
    "        buy = True\n",
    "        profit.append(usd - capital)\n",
    "        print(\"Profit: %f\" % profit[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final profit: $72.36\n"
     ]
    }
   ],
   "source": [
    "print(\"Final profit: $%.02f\" % profit[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('AMZN.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('AMZN-1.hd5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
